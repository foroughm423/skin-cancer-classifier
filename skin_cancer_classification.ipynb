{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¬ Skin Cancer Classification â€“ EfficientNetB3\n",
        "\n",
        "**Training notebook for the deployed Streamlit application.**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Quick Results\n",
        "\n",
        "- **Test Accuracy:** 77.13%\n",
        "- **Melanoma Recall:** 63.68%\n",
        "- **Macro AUC:** 0.94\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”— Links\n",
        "\n",
        "- **ðŸ“– Full Documentation:** [README.md](../README.md)\n",
        "- **ðŸš€ Live Demo:** [Streamlit App](https://skin-cancer-classifier-mkzevixv7y2x2wmrnlvaw3.streamlit.app/)\n",
        "- **ðŸ““ Notebook with Outputs:** [Kaggle](https://www.kaggle.com/code/foroughgh95/skin-cancer-efficientnetb3-ham10000)\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This notebook has outputs removed for cleaner version control. For full training logs and visualizations, see the [Kaggle version](https://www.kaggle.com/code/foroughgh95/skin-cancer-efficientnetb3-ham10000) or refer to [README.md](../README.md) for complete project details.\n",
        "\n",
        "---\n",
        "\n",
        "**Model:** EfficientNetB3 | **Loss:** Focal Loss | **Framework:** TensorFlow 2.19"
      ],
      "metadata": {
        "id": "piuS7FiKOMhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Core utilities\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "hD71dIfkQk4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Setup Instructions\n",
        "\n",
        "# This notebook uses the HAM10000 dataset from Kaggle:\n",
        "# https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000\n",
        "#\n",
        "# To run locally:\n",
        "# 1. Download the dataset from the link above\n",
        "# 2. Extract it to: ./skin_cancer_data/\n",
        "# 3. Ensure the following files/folders exist:\n",
        "#    - HAM10000_metadata.csv\n",
        "#    - HAM10000_images_part_1/\n",
        "#    - HAM10000_images_part_2/\n",
        "#\n",
        "# The download script below is for Kaggle/Colab only:\n",
        "#\n",
        "# !pip install -q kaggle\n",
        "# !kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "# !unzip -q skin-cancer-mnist-ham10000.zip -d ./skin_cancer_data\n",
        "\n",
        "print(\"Dataset should be in ./skin_cancer_data\")"
      ],
      "metadata": {
        "id": "HAdPh_VNQmvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Prepare Metadata\n",
        "\n",
        "# Load dataset metadata\n",
        "df = pd.read_csv('./skin_cancer_data/HAM10000_metadata.csv')\n",
        "\n",
        "# Map diagnosis codes to full lesion names\n",
        "lesion_labels = {\n",
        "    'nv': 'Melanocytic nevus',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratosis',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "df['lesion_type'] = df['dx'].map(lesion_labels)\n",
        "\n",
        "# Create numeric labels with fixed order for consistency\n",
        "label_order = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "df['label_idx'] = df['dx'].astype('category').cat.reorder_categories(label_order).cat.codes\n",
        "\n",
        "# Define image directories (case-insensitive for cross-platform compatibility)\n",
        "IMAGE_DIRS = [\n",
        "    './skin_cancer_data/HAM10000_images_part_1',\n",
        "    './skin_cancer_data/HAM10000_images_part_2',\n",
        "    './skin_cancer_data/ham10000_images_part_1',\n",
        "    './skin_cancer_data/ham10000_images_part_2'\n",
        "]\n",
        "\n",
        "# Resolve image paths\n",
        "def get_image_path(image_id):\n",
        "    for dir_path in IMAGE_DIRS:\n",
        "        path = os.path.join(dir_path, f'{image_id}.jpg')\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "df['path'] = df['image_id'].apply(get_image_path)\n",
        "\n",
        "# Verify data integrity\n",
        "missing_count = df['path'].isna().sum()\n",
        "if missing_count > 0:\n",
        "    print(f\"Warning: {missing_count} images not found in dataset folders.\")\n",
        "print(f\"Dataset loaded successfully\")\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Missing paths: {missing_count}\")"
      ],
      "metadata": {
        "id": "Ci3JefLNQ6uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Class Distribution\n",
        "\n",
        "# Display class distribution\n",
        "print(\"\\nLesion Type Distribution:\")\n",
        "print(df['dx'].value_counts())\n",
        "\n",
        "# Plot distribution (sorted by frequency)\n",
        "plt.figure(figsize=(10, 6))\n",
        "order = df['lesion_type'].value_counts().index\n",
        "sns.countplot(data=df, x='lesion_type', order=order)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Lesion Type')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Lesion Types')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pg7rccNtRBqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Sample Images per Lesion Type\n",
        "\n",
        "# Define image directories (case-insensitive)\n",
        "image_dirs = [\n",
        "    './skin_cancer_data/HAM10000_images_part_1',\n",
        "    './skin_cancer_data/HAM10000_images_part_2',\n",
        "    './skin_cancer_data/ham10000_images_part_1',\n",
        "    './skin_cancer_data/ham10000_images_part_2'\n",
        "]\n",
        "\n",
        "classes = df['lesion_type'].unique()\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, lesion in enumerate(classes):\n",
        "    sample = df[df['lesion_type'] == lesion].sample(1, random_state=42).iloc[0]\n",
        "    image_filename = sample['image_id'] + '.jpg'\n",
        "\n",
        "    for dir_path in image_dirs:\n",
        "        image_path = os.path.join(dir_path, image_filename)\n",
        "        if os.path.exists(image_path):\n",
        "            img = mpimg.imread(image_path)\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Image {image_filename} not found in either directory.\")\n",
        "        continue\n",
        "\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(lesion, fontweight='bold', fontsize=10)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S55CCK0gRC9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset into Train, Validation, and Test Sets\n",
        "\n",
        "# Split: Train+Val (80%) vs Test (20%)\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['label_idx'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split: Train (60%) vs Validation (20%) from Train+Val\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.25,  # 25% of 80% = 20% of total\n",
        "    stratify=train_val_df['label_idx'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Display split sizes\n",
        "total_samples = len(df)\n",
        "print(f\"Train samples: {len(train_df)} ({len(train_df)/total_samples*100:.1f}%)\")\n",
        "print(f\"Validation samples: {len(val_df)} ({len(val_df)/total_samples*100:.1f}%)\")\n",
        "print(f\"Test samples: {len(test_df)} ({len(test_df)/total_samples*100:.1f}%)\")\n",
        "\n",
        "# Verify class distribution in each split\n",
        "print(\"\\nClass distribution in each split:\")\n",
        "for name, subset in [(\"Train\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
        "    print(f\"{name}:\")\n",
        "    distribution = subset['dx'].value_counts()\n",
        "    total_subset = len(subset)\n",
        "    for dx, count in distribution.items():\n",
        "        percent = (count / total_subset) * 100\n",
        "        print(f\"{dx}: {count} ({percent:.2f}%)\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "LDej1FxRRJls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generators with Optimized Augmentation\n",
        "\n",
        "# Hyperparameters\n",
        "TARGET_SIZE = (300, 300)      # EfficientNetB3 optimal input\n",
        "BATCH_SIZE = 24               # Balanced for memory and gradient stability\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Moderate augmentation (domain-aware)\n",
        "ROTATION_RANGE = 20\n",
        "WIDTH_SHIFT_RANGE = 0.15\n",
        "HEIGHT_SHIFT_RANGE = 0.15\n",
        "SHEAR_RANGE = 0.1\n",
        "ZOOM_RANGE = 0.2\n",
        "BRIGHTNESS_RANGE = [0.8, 1.2]\n",
        "HORIZONTAL_FLIP = True\n",
        "VERTICAL_FLIP = False\n",
        "CHANNEL_SHIFT_RANGE = 0.1\n",
        "\n",
        "# Train generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=ROTATION_RANGE,\n",
        "    width_shift_range=WIDTH_SHIFT_RANGE,\n",
        "    height_shift_range=HEIGHT_SHIFT_RANGE,\n",
        "    shear_range=SHEAR_RANGE,\n",
        "    horizontal_flip=HORIZONTAL_FLIP,\n",
        "    vertical_flip=VERTICAL_FLIP,\n",
        "    zoom_range=ZOOM_RANGE,\n",
        "    brightness_range=BRIGHTNESS_RANGE,\n",
        "    channel_shift_range=CHANNEL_SHIFT_RANGE,\n",
        "    fill_mode='reflect'\n",
        ")\n",
        "\n",
        "# Validation & Test (no augmentation)\n",
        "val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='path',\n",
        "    y_col='label_idx',\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='raw',\n",
        "    shuffle=True,\n",
        "    seed=RANDOM_STATE\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='path',\n",
        "    y_col='label_idx',\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='raw',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='path',\n",
        "    y_col='label_idx',\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='raw',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Verify generator setup\n",
        "batch_x, batch_y = next(train_generator)\n",
        "print(f\"Generators created successfully\")\n",
        "print(f\"Train batches: {len(train_generator)}\")\n",
        "print(f\"Val batches: {len(val_generator)}\")\n",
        "print(f\"Test batches: {len(test_generator)}\")\n",
        "print(f\"Batch shape: {batch_x.shape}\")\n",
        "print(f\"Image range: [{batch_x.min():.2f}, {batch_x.max():.2f}]\")"
      ],
      "metadata": {
        "id": "4ahK2u8ERNZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Compile Model (B3 Optimized)\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_SHAPE = (300, 300, 3)\n",
        "DENSE_UNITS = 384\n",
        "DROPOUT_RATE = 0.3\n",
        "LEARNING_RATE = 0.0005\n",
        "ACTIVATION = 'swish'\n",
        "\n",
        "# Focal Loss (optimized for class imbalance)\n",
        "def focal_loss(gamma=2.0, alpha=None):\n",
        "    if alpha is None:\n",
        "        # Class weights: [akiec, bcc, bkl, df, mel, nv, vasc]\n",
        "        alpha = [0.5, 0.3, 0.3, 1.2, 0.55, 0.25, 1.5]\n",
        "\n",
        "    alpha_tensor = tf.constant(alpha, dtype=tf.float32)\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        if len(y_true.shape) > 1:\n",
        "            y_true = tf.squeeze(y_true, axis=-1)\n",
        "\n",
        "        y_true_one_hot = tf.one_hot(y_true, depth=7, dtype=tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
        "\n",
        "        alpha_t = tf.reduce_sum(y_true_one_hot * alpha_tensor, axis=-1, keepdims=True)\n",
        "        ce = -y_true_one_hot * tf.math.log(y_pred)\n",
        "        weight = tf.pow(1 - y_pred, gamma)\n",
        "        fl = alpha_t * weight * ce\n",
        "\n",
        "        return tf.reduce_mean(tf.reduce_sum(fl, axis=-1))\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# Load EfficientNet-B3 (pretrained)\n",
        "base_model = EfficientNetB3(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=INPUT_SHAPE\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n",
        "print(f\"EfficientNetB3 loaded: {len(base_model.layers)} layers\")\n",
        "\n",
        "# Custom classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D(name='gap')(x)\n",
        "x = Dense(DENSE_UNITS, activation=ACTIVATION, name='dense1')(x)\n",
        "x = BatchNormalization(name='bn1')(x)\n",
        "x = Dropout(DROPOUT_RATE, name='dropout1')(x)\n",
        "x = Dense(128, activation=ACTIVATION, name='dense2')(x)\n",
        "x = BatchNormalization(name='bn2')(x)\n",
        "x = Dropout(DROPOUT_RATE * 0.7, name='dropout2')(x)\n",
        "predictions = Dense(7, activation='softmax', name='output')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile with Focal Loss\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=focal_loss(gamma=2.0, alpha=None),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model info\n",
        "print(f\"Model compiled: {model.count_params():,} total parameters\")\n",
        "trainable = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
        "print(f\"Trainable: {trainable:,}, Frozen: {model.count_params() - trainable:,}\")"
      ],
      "metadata": {
        "id": "r2BugCsCRQMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Model (Single-Phase, Frozen Base)\n",
        "\n",
        "# Hyperparameters\n",
        "TOTAL_EPOCHS = 45\n",
        "STEPS_PER_EPOCH = 1.0\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "REDUCE_LR_PATIENCE = 5\n",
        "REDUCE_LR_FACTOR = 0.5\n",
        "\n",
        "# Cosine annealing learning rate schedule\n",
        "def cosine_annealing_schedule(epoch, lr, initial_lr=0.0005, min_lr=1e-7, epochs=45):\n",
        "    if epoch < epochs:\n",
        "        return min_lr + (initial_lr - min_lr) * (1 + math.cos(math.pi * epoch / epochs)) / 2\n",
        "    return min_lr\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_path = 'efficientnetb3_ham10000_model.keras'  # Save path for best model (standardized filename for deployment)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=EARLY_STOPPING_PATIENCE,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=REDUCE_LR_FACTOR,\n",
        "    patience=REDUCE_LR_PATIENCE,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(\n",
        "    lambda epoch: cosine_annealing_schedule(epoch, model.optimizer.learning_rate.numpy(), epochs=TOTAL_EPOCHS),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SINGLE PHASE TRAINING (No Fine-tuning)\")\n",
        "print(\"Strategy: Keep base model frozen, train only custom head\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=int(len(train_generator) * STEPS_PER_EPOCH),\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=int(len(val_generator) * STEPS_PER_EPOCH),\n",
        "    epochs=TOTAL_EPOCHS,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "best_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "print(f\"Best Val Accuracy: {best_val_acc * 100:.2f}% (Epoch {best_epoch})\")\n",
        "print(f\"Final Val Accuracy: {final_val_acc * 100:.2f}%\")\n",
        "print(f\"Model saved: {checkpoint_path}\")\n",
        "\n",
        "# # Download best model\n",
        "# files.download(checkpoint_path)"
      ],
      "metadata": {
        "id": "0KIDsJg-RTWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Training History\n",
        "\n",
        "# Extract metrics\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Create plots\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, [a * 100 for a in acc], label='Training Accuracy')\n",
        "plt.plot(epochs, [a * 100 for a in val_acc], label='Validation Accuracy')\n",
        "best_epoch = np.argmax(val_acc) + 1\n",
        "plt.axvline(best_epoch, color='r', linestyle='--', label=f'Best Epoch = {best_epoch}')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.axvline(best_epoch, color='r', linestyle='--')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2J1qLSjVRYeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model on Test Data\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predictions\n",
        "test_generator.reset()\n",
        "y_pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_df['label_idx'].values\n",
        "\n",
        "# Class names (full)\n",
        "target_names = [lesion_labels[c] for c in label_order]\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=target_names, digits=4))\n",
        "\n",
        "# Confusion Matrix (absolute)\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h4CN-sunRap6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC Curves\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(label_order)\n",
        "\n",
        "# Convert true labels to one-hot for ROC\n",
        "y_test_bin = label_binarize(y_true, classes=range(num_classes))\n",
        "\n",
        "# Use prediction probabilities from previous cell\n",
        "y_pred_probs = y_pred\n",
        "\n",
        "# Compute ROC curve and AUC for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Micro-average ROC\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_probs.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Macro-average ROC\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(num_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "mean_tpr /= num_classes\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple', 'brown']\n",
        "for i, color in zip(range(num_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label=f'{lesion_labels[label_order[i]]} (AUC = {roc_auc[i]:0.2f})')\n",
        "\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', linewidth=3,\n",
        "         label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.2f})')\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle=':', linewidth=3,\n",
        "         label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Guess')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.title('ROC Curves (One-vs-Rest)', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R17Y7tHMJlAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}